# Persona-Driven Document Intelligence Pipeline  
Adobe India Hackathon 2025 — "Connecting the Dots" Challenge

---

## Overview

This solution transforms raw PDFs into a personalized, structured knowledge resource tailored to user personas and their specific tasks. It comprises multiple integrated modules that collectively:

- Parse PDFs to identify titles and hierarchical headings (PDF Parser, Round 1A)
- Extract meaningful text chunks per section and summarize them (Analysis Engine)
- Rank and select the most relevant sections guided by a persona and job description (Output Generator)
- Orchestrate the entire process seamlessly from data ingestion to formatted JSON results (Main)

---

## System Components

### 1. PDF Parser (Round 1A)

- Utilizes `PyMuPDF` to read PDFs and extract layout information such as font size, text style (bold/italic), and positioning.
- Detects document title and hierarchical section headings (H1, H2, H3) based on typography and numeric patterns.
- Outputs clean outline JSON files per PDF with structured heading information.

### 2. Analysis Engine

- Converts textual content of each section into semantic embeddings using localized `sentence-transformers` models.
- Leverages heuristic scoring combining semantic similarity with persona/job embeddings, heading confidence, and document diversity to assign relevance scores.
- Extracts refined subsection summaries by selecting key sentences in context, emphasizing important keywords and sentence position.

### 3. Output Generator

- Aggregates analyzed and ranked sections from all documents in the collection.
- Produces final JSON output conforming to the hackathon schema, including:
  - Metadata with persona, job, list of documents, and processing timestamp
  - Ranked list of extracted sections with document and page references
  - Subsection analysis entries with curated text snippets for enhanced user understanding

### 4. Main Orchestrator

- Coordinates the workflow:
  - Runs PDF parsing to generate outlines if missing
  - Creates or refreshes subsection summaries
  - Performs semantic analysis and ranking
  - Compiles and saves the final result JSON

---

## Directory Structure

Your pipeline expects the following folder hierarchy (relative to the root working directory):

```
CHALLENGE_1B/
├── Collection X/
│   ├── PDFs/                    # Place input PDF files here
│   ├── (outline JSONs)          # Auto-generated by the parser, one per PDF
│   ├── subsection_analysis.json # Generated by Analysis Engine
│   ├── challengeX_input.json    # Persona, job, and document list for analysis
│   ├── challengeX_output.json   # Generated final output after ranking
├── pdf_parser.py                # Module implementing the Round 1A logic
├── analysis_engine.py           # Module encapsulating semantic scoring & summarization
├── output_generator.py          # Module handling result aggregation and writing
├── main.py                      # Orchestrator script integrating all components
├── requirements.txt             # Python dependencies
├── README.md                    # This documentation
```

---

## Setup & Installation

### Dependencies

All dependencies are installable via PyPI:

```
pip install -r requirements.txt
```

Typical contents of `requirements.txt` include:

```
pymupdf
sentence-transformers
scikit-learn
nltk
```

### Model Preparation (Offline)

Since internet access is disabled during execution:

- Pre-download the `sentence-transformers` model (`all-MiniLM-L6`) on a machine with internet:

```
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
```

- Copy the downloaded model directory from the cache (`~/.cache/torch/sentence_transformers/` or equivalent) to the offline environment.

- In the code, ensure models load with `local_files_only=True` to prevent any network calls.

---

## Usage Instructions

1. **Place** your PDF files into the appropriate collection’s `PDFs/` folder.

2. **Prepare** `challengeX_input.json` in the collection folder including:

```
{
  "persona": { "role": "Travel Planner" },
  "job": { "task": "Plan a 4-day trip for college friends" },
  "documents": [
    { "filename": "South of France - Cities.pdf" },
    ...
  ]
}
```

3. **Configure** `main.py`:

```
COLLECTION_NAME = 'Collection 1'   # Adjust as needed
```

4. **Run the pipeline:**

```
python main.py
```

5. **Results:**

- **Per-PDF outlines** generated as `*.json` next to PDFs.
- **`subsection_analysis.json`** created per collection.
- **Final ranked output** saved as `challengeX_output.json` for easy evaluation/submission.

---

## Output Format Highlights

Final output JSON includes:

- **metadata**: Documents involved, persona role, job description, timestamp.

- **extracted_sections**: Ranked list of sections with document name, heading text, page, and rank.

- **subsection_analysis**: Curated textual snippets for highlighted sections, providing quick insights.

